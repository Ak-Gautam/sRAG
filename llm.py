# llm.py

import logging
from typing import List, Optional, Dict, Any

import torch
from transformers import AutoModelForCausalLM, AutoModelForSeq2SeqLM, AutoTokenizer, StoppingCriteriaList, StoppingCriteria
from ChunkNode import Node
from EmbeddingsIndex import EmbeddingIndexer

# Configure logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


class StopOnTokens(StoppingCriteria):
    """
    Custom stopping criteria for text generation to stop when any of the stop tokens is generated.
    """

    def __init__(self, stop_tokens: List[str], tokenizer):
        self.stop_tokens = stop_tokens
        self.tokenizer = tokenizer

    def __call__(self, input_ids, scores, **kwargs):
        decoded_text = self.tokenizer.decode(input_ids[0], skip_special_tokens=True)
        for stop_token in self.stop_tokens:
            if stop_token in decoded_text:
                return True
        return False


class LLMInterface:
    """
    A class that handles interaction with Hugging Face language models for text generation.

    Attributes:
        model_name (str): The name of the Hugging Face model to use.
        device (str): The device to run the model on ('cpu' or 'cuda').
        tokenizer (AutoTokenizer): The tokenizer associated with the model.
        model (AutoModelForCausalLM or AutoModelForSeq2SeqLM): The language model loaded from Hugging Face.
        max_input_length (int): Maximum allowed input length for the model.
        max_output_length (int): Maximum allowed output length generated by the model.
        is_encoder_decoder (bool): Flag to determine if the model is encoder-decoder (e.g., T5) or decoder-only (e.g., GPT-2).
    """

    def __init__(
        self,
        model_name: str = "unsloth/Llama-3.2-1B",
        device: Optional[str] = None,
        max_input_length: int = 1024,
        max_output_length: int = 256,
    ):
        """
        Initializes the LLMInterface with the specified parameters.

        Args:
            model_name (str, optional): Hugging Face model name. Defaults to "gpt2".
            device (str, optional): Device to run the model on ('cpu' or 'cuda'). Defaults to 'cuda' if available.
            max_input_length (int, optional): Maximum input token length. Defaults to 1024.
            max_output_length (int, optional): Maximum output token length. Defaults to 256.
        """
        self.model_name = model_name
        self.device = device if device else ("cuda" if torch.cuda.is_available() else "cpu")
        self.max_input_length = max_input_length
        self.max_output_length = max_output_length

        logger.info(f"Initializing LLMInterface with model: {self.model_name}")
        self.tokenizer = AutoTokenizer.from_pretrained(self.model_name)

        # Check if the model is encoder-decoder or decoder-only
        try:
            self.model = AutoModelForSeq2SeqLM.from_pretrained(self.model_name).to(self.device)
            self.is_encoder_decoder = True
            logger.info("Loaded encoder-decoder model.")
        except Exception:
            self.model = AutoModelForCausalLM.from_pretrained(self.model_name).to(self.device)
            self.is_encoder_decoder = False
            logger.info("Loaded decoder-only model.")

        self.model.eval()

    def generate(
        self,
        prompt: str,
        max_length: Optional[int] = None,
        stop_tokens: Optional[List[str]] = None,
        **generate_kwargs,
    ) -> str:
        """
        Generates text based on the given prompt.

        Args:
            prompt (str): The input prompt for the model.
            max_length (int, optional): Maximum length of the generated output. Defaults to self.max_output_length.
            stop_tokens (List[str], optional): List of tokens where generation will stop. Defaults to None.
            **generate_kwargs: Additional keyword arguments for the generate method.

        Returns:
            str: The generated text output.
        """
        max_output_length = max_length if max_length else self.max_output_length

        logger.info(f"Generating text for prompt with length {len(prompt)} characters.")

        # Encode the prompt
        inputs = self.tokenizer(
            prompt,
            return_tensors="pt",
            truncation=True,
            max_length=self.max_input_length,
            add_special_tokens=True,
        ).to(self.device)

        input_length = inputs.input_ids.shape[-1]
        total_max_length = input_length + max_output_length

        # Prepare stopping criteria
        stopping_criteria = StoppingCriteriaList()
        if stop_tokens:
            stopping_criteria.append(StopOnTokens(stop_tokens, self.tokenizer))

        with torch.no_grad():
            if self.is_encoder_decoder:
                outputs = self.model.generate(
                    **inputs,
                    max_length=total_max_length,
                    num_beams=generate_kwargs.get("num_beams", 1),
                    early_stopping=True,
                    stopping_criteria=stopping_criteria or None,
                    **generate_kwargs,
                )
            else:
                outputs = self.model.generate(
                    **inputs,
                    max_length=total_max_length,
                    pad_token_id=self.tokenizer.eos_token_id,
                    do_sample=generate_kwargs.get("do_sample", True),
                    temperature=generate_kwargs.get("temperature", 0.7),
                    top_k=generate_kwargs.get("top_k", 50),
                    top_p=generate_kwargs.get("top_p", 0.95),
                    stopping_criteria=stopping_criteria or None,
                    **generate_kwargs,
                )

        generated_text = self.tokenizer.decode(outputs[0], skip_special_tokens=True)

        # Remove the prompt from the generated text if necessary
        if generated_text.startswith(prompt):
            generated_text = generated_text[len(prompt):]

        logger.info("Text generation completed.")
        return generated_text.strip()

    def generate_dataset(
        self,
        nodes: List[Node],
        examples: Optional[List[Dict[str, Any]]] = None,
        prompt_template: str = "{context}\n\n{instruction}",
        instruction_key: str = "instruction",
        output_key: str = "output",
        **generate_kwargs,
    ) -> List[Dict[str, Any]]:
        """
        Generates a dataset using the knowledge from nodes and an example dataset.

        Args:
            nodes (List[Node]): List of nodes containing text and metadata.
            examples (List[Dict[str, Any]], optional): Example dataset with instructions. Defaults to None.
            prompt_template (str, optional): Template for constructing prompts. Defaults to "{context}\n\n{instruction}".
            instruction_key (str, optional): Key for instruction in examples. Defaults to "instruction".
            output_key (str, optional): Key for output in examples. Defaults to "output".
            **generate_kwargs: Additional keyword arguments for the generate method.

        Returns:
            List[Dict[str, Any]]: Generated dataset in the same format as examples.
        """
        if not examples:
            logger.warning("No examples provided. Using nodes for context only.")
            examples = []

        generated_dataset = []
        logger.info(f"Starting dataset generation using {len(nodes)} nodes.")

        for idx, node in enumerate(nodes):
            context = node.text
            instruction = examples[idx % len(examples)].get(instruction_key, "Provide a summary of the above text.")
            prompt = prompt_template.format(context=context, instruction=instruction)

            logger.info(f"Generating output for node {idx+1}/{len(nodes)}.")
            output = self.generate(prompt, **generate_kwargs)

            generated_entry = {
                instruction_key: instruction,
                output_key: output,
                "metadata": node.metadata,
            }
            generated_dataset.append(generated_entry)

        logger.info("Dataset generation completed.")
        return generated_dataset

    def close(self):
        """
        Closes any open resources.
        """
        pass  # Currently, there are no persistent connections to close

    def __del__(self):
        self.close()